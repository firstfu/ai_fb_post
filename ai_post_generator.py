"""
AI è²¼æ–‡ç”Ÿæˆå™¨ - ä½¿ç”¨ LangGraph ç”Ÿæˆé«˜è½‰åŒ–ç‡çš„ Facebook è²¼æ–‡å’Œé…åœ–

æ­¤æ¨¡çµ„æä¾›å®Œæ•´çš„ AI é©…å‹•è²¼æ–‡ç”Ÿæˆç³»çµ±ï¼š
- ğŸ¤– ä½¿ç”¨ LangGraph å·¥ä½œæµç¨‹ç”Ÿæˆé«˜è½‰åŒ–ç‡è²¼æ–‡å…§å®¹
- ğŸ–¼ï¸ AI ç”Ÿæˆé…åœ–åŠŸèƒ½
- ğŸ“Š è²¼æ–‡æ•ˆæœå„ªåŒ–å»ºè­°
- ğŸ¯ å¤šç¨®è²¼æ–‡é¡å‹æ”¯æ´
- ğŸ”§ å¯è‡ªå®šç¾©ç”Ÿæˆåƒæ•¸

ä¸»è¦åŠŸèƒ½ï¼š
- æ™ºèƒ½è²¼æ–‡å…§å®¹ç”Ÿæˆ
- é…åœ–ç”Ÿæˆå’Œå„ªåŒ–
- è²¼æ–‡æ•ˆæœé æ¸¬
- å¤šèªè¨€æ”¯æ´
- å“ç‰Œé¢¨æ ¼é©é…

æŠ€è¡“æ¶æ§‹ï¼š
- LangGraph å·¥ä½œæµç¨‹å¼•æ“
- LangChain AI æ¡†æ¶
- OpenAI GPT æ¨¡å‹
- DALL-E 3 åœ–åƒç”Ÿæˆ
- Pydantic æ•¸æ“šé©—è­‰

@fileoverview AI è²¼æ–‡ç”Ÿæˆç³»çµ±æ ¸å¿ƒæ¨¡çµ„
@version 1.0.0
@author AI Assistant
@created 2024-01-20

@requires langchain>=0.2.0
@requires langgraph>=0.2.0
@requires openai>=1.0.0
@requires pillow>=10.0.0
"""

import os
import json
import base64
import asyncio
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path
from io import BytesIO

import streamlit as st
from PIL import Image
import requests
from pydantic import BaseModel, Field

# LangChain å’Œ LangGraph ç›¸é—œå°å…¥
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain.schema import BaseMessage, AIMessage, HumanMessage, SystemMessage
from langgraph.graph import StateGraph, END
from typing import TypedDict

# é…ç½®æ–‡ä»¶è·¯å¾‘
AI_CONFIG_FILE = Path("data/ai_config.json")
GENERATED_IMAGES_DIR = Path("data/generated_images")

# ç¢ºä¿ç›®éŒ„å­˜åœ¨
GENERATED_IMAGES_DIR.mkdir(parents=True, exist_ok=True)

class PostGenerationRequest(BaseModel):
    """è²¼æ–‡ç”Ÿæˆè«‹æ±‚æ¨¡å‹"""
    topic: str = Field(..., description="è²¼æ–‡ä¸»é¡Œ")
    target_audience: str = Field(default="ä¸€èˆ¬å¤§çœ¾", description="ç›®æ¨™å—çœ¾")
    post_type: str = Field(default="æ¨å»£", description="è²¼æ–‡é¡å‹")
    tone: str = Field(default="å‹å–„è¦ªåˆ‡", description="èªèª¿é¢¨æ ¼")
    include_hashtags: bool = Field(default=True, description="æ˜¯å¦åŒ…å«æ¨™ç±¤")
    include_emoji: bool = Field(default=True, description="æ˜¯å¦åŒ…å«è¡¨æƒ…ç¬¦è™Ÿ")
    max_length: int = Field(default=300, description="æœ€å¤§å­—æ•¸")
    generate_image: bool = Field(default=False, description="æ˜¯å¦ç”Ÿæˆé…åœ–")
    image_style: str = Field(default="ç¾ä»£ç°¡ç´„", description="é…åœ–é¢¨æ ¼")

class GeneratedPost(BaseModel):
    """ç”Ÿæˆçš„è²¼æ–‡æ¨¡å‹"""
    title: str
    content: str
    hashtags: List[str] = []
    predicted_engagement: Dict[str, float] = {}
    optimization_tips: List[str] = []
    image_url: Optional[str] = None
    image_description: Optional[str] = None
    generation_metadata: Dict[str, Any] = {}

class AIPostState(TypedDict):
    """AI è²¼æ–‡ç”Ÿæˆç‹€æ…‹"""
    request: PostGenerationRequest
    generated_content: str
    generated_title: str
    hashtags: List[str]
    optimization_tips: List[str]
    engagement_prediction: Dict[str, float]
    image_prompt: Optional[str]
    image_url: Optional[str]
    image_description: Optional[str]
    errors: List[str]

class AIConfigManager:
    """AI é…ç½®ç®¡ç†å™¨"""

    @staticmethod
    def load_config() -> Dict[str, Any]:
        """è¼‰å…¥ AI é…ç½®"""
        default_config = {
            "openai_api_key": "",
            "anthropic_api_key": "",
            "default_model": "gpt-4o-mini",
            "image_model": "dall-e-3",
            "max_tokens": 1000,
            "temperature": 0.7,
            "enable_image_generation": True
        }

        if not AI_CONFIG_FILE.exists():
            AIConfigManager.save_config(default_config)
            return default_config

        try:
            with open(AI_CONFIG_FILE, 'r', encoding='utf-8') as f:
                config = json.load(f)
                # åˆä½µé è¨­é…ç½®ä»¥ç¢ºä¿æ‰€æœ‰å¿…è¦çš„éµéƒ½å­˜åœ¨
                return {**default_config, **config}
        except Exception:
            return default_config

    @staticmethod
    def save_config(config: Dict[str, Any]):
        """ä¿å­˜ AI é…ç½®"""
        try:
            with open(AI_CONFIG_FILE, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
        except Exception as e:
            st.error(f"ä¿å­˜ AI é…ç½®å¤±æ•—: {e}")

class PostContentGenerator:
    """è²¼æ–‡å…§å®¹ç”Ÿæˆå™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.llm = self._init_llm()

    def _init_llm(self):
        """åˆå§‹åŒ–èªè¨€æ¨¡å‹"""
        model_name = self.config.get("default_model", "gpt-4o-mini")

        if model_name.startswith("gpt-"):
            if not self.config.get("openai_api_key"):
                raise ValueError("éœ€è¦è¨­ç½® OpenAI API Key")
            return ChatOpenAI(
                model=model_name,
                api_key=self.config["openai_api_key"],
                temperature=self.config.get("temperature", 0.7),
                max_tokens=self.config.get("max_tokens", 1000)
            )
        elif model_name.startswith("claude-"):
            if not self.config.get("anthropic_api_key"):
                raise ValueError("éœ€è¦è¨­ç½® Anthropic API Key")
            return ChatAnthropic(
                model=model_name,
                api_key=self.config["anthropic_api_key"],
                temperature=self.config.get("temperature", 0.7),
                max_tokens=self.config.get("max_tokens", 1000)
            )
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡å‹: {model_name}")

    def generate_title(self, state: AIPostState) -> AIPostState:
        """ç”Ÿæˆè²¼æ–‡æ¨™é¡Œ"""
        try:
            request = state["request"]

            prompt = ChatPromptTemplate.from_messages([
                SystemMessagePromptTemplate.from_template(
                    "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¤¾ç¾¤åª’é«”å…§å®¹å‰µä½œå°ˆå®¶ï¼Œå°ˆé–€å‰µä½œé«˜è½‰åŒ–ç‡çš„ Facebook è²¼æ–‡æ¨™é¡Œã€‚"
                    "è«‹æ ¹æ“šä»¥ä¸‹è¦æ±‚ç”Ÿæˆå¸å¼•äººçš„æ¨™é¡Œã€‚"
                ),
                HumanMessagePromptTemplate.from_template(
                    "è«‹ç‚ºä»¥ä¸‹ä¸»é¡Œå‰µä½œä¸€å€‹å¸å¼•äººçš„ Facebook è²¼æ–‡æ¨™é¡Œï¼š\n\n"
                    "ä¸»é¡Œï¼š{topic}\n"
                    "ç›®æ¨™å—çœ¾ï¼š{target_audience}\n"
                    "è²¼æ–‡é¡å‹ï¼š{post_type}\n"
                    "èªèª¿é¢¨æ ¼ï¼š{tone}\n"
                    "æ˜¯å¦ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿï¼š{include_emoji}\n\n"
                    "è¦æ±‚ï¼š\n"
                    "1. æ¨™é¡Œè¦ç°¡æ½”æœ‰åŠ›ï¼Œèƒ½å¤ å¸å¼•é»æ“Š\n"
                    "2. ç¬¦åˆç›®æ¨™å—çœ¾çš„èˆˆè¶£å’Œéœ€æ±‚\n"
                    "3. é«”ç¾æŒ‡å®šçš„èªèª¿é¢¨æ ¼\n"
                    "4. å­—æ•¸æ§åˆ¶åœ¨ 50 å­—ä»¥å…§\n"
                    "5. ä½¿ç”¨ç¹é«”ä¸­æ–‡\n\n"
                    "åªå›å‚³æ¨™é¡Œå…§å®¹ï¼Œä¸éœ€è¦å…¶ä»–èªªæ˜ã€‚"
                )
            ])

            formatted_prompt = prompt.format_messages(
                topic=request.topic,
                target_audience=request.target_audience,
                post_type=request.post_type,
                tone=request.tone,
                include_emoji="æ˜¯" if request.include_emoji else "å¦"
            )

            response = self.llm.invoke(formatted_prompt)
            state["generated_title"] = response.content.strip()

        except Exception as e:
            state["errors"].append(f"æ¨™é¡Œç”Ÿæˆå¤±æ•—: {str(e)}")
            state["generated_title"] = f"é—œæ–¼{state['request'].topic}çš„ç²¾å½©å…§å®¹"

        return state

    def generate_content(self, state: AIPostState) -> AIPostState:
        """ç”Ÿæˆè²¼æ–‡å…§å®¹"""
        try:
            request = state["request"]

            prompt = ChatPromptTemplate.from_messages([
                SystemMessagePromptTemplate.from_template(
                    "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¤¾ç¾¤åª’é«”å…§å®¹å‰µä½œå°ˆå®¶ï¼Œå°ˆé–€å‰µä½œé«˜è½‰åŒ–ç‡çš„ Facebook è²¼æ–‡ã€‚"
                    "ä½ äº†è§£å¦‚ä½•é‹ç”¨å¿ƒç†å­¸åŸç†ã€storytelling æŠ€å·§å’Œç¤¾ç¾¤åª’é«”æœ€ä½³å¯¦è¸ä¾†å‰µä½œå¸å¼•äººçš„å…§å®¹ã€‚"
                ),
                HumanMessagePromptTemplate.from_template(
                    "è«‹ç‚ºä»¥ä¸‹ä¸»é¡Œå‰µä½œä¸€ç¯‡é«˜è½‰åŒ–ç‡çš„ Facebook è²¼æ–‡å…§å®¹ï¼š\n\n"
                    "ä¸»é¡Œï¼š{topic}\n"
                    "ç›®æ¨™å—çœ¾ï¼š{target_audience}\n"
                    "è²¼æ–‡é¡å‹ï¼š{post_type}\n"
                    "èªèª¿é¢¨æ ¼ï¼š{tone}\n"
                    "æœ€å¤§å­—æ•¸ï¼š{max_length}\n"
                    "æ˜¯å¦ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿï¼š{include_emoji}\n\n"
                    "è«‹éµå¾ªä»¥ä¸‹åŸå‰‡ï¼š\n"
                    "1. é–‹é ­è¦æœ‰å¸å¼•åŠ›ï¼Œèƒ½å¤ ç«‹å³æŠ“ä½è®€è€…æ³¨æ„åŠ›\n"
                    "2. å…§å®¹è¦æœ‰åƒ¹å€¼ï¼Œå°ç›®æ¨™å—çœ¾æœ‰å¯¦éš›å¹«åŠ©\n"
                    "3. ä½¿ç”¨é©ç•¶çš„æƒ…æ„Ÿè§¸ç™¼é»\n"
                    "4. åŒ…å«æ˜ç¢ºçš„è¡Œå‹•å‘¼ç±² (CTA)\n"
                    "5. çµæ§‹æ¸…æ™°ï¼Œæ˜“æ–¼é–±è®€\n"
                    "6. ç¬¦åˆæŒ‡å®šçš„èªèª¿é¢¨æ ¼\n"
                    "7. ä½¿ç”¨ç¹é«”ä¸­æ–‡\n"
                    "8. å¦‚æœé©åˆï¼Œå¯ä»¥åŠ å…¥ç›¸é—œçš„æ•…äº‹æˆ–æ¡ˆä¾‹\n\n"
                    "åªå›å‚³è²¼æ–‡å…§å®¹ï¼Œä¸éœ€è¦å…¶ä»–èªªæ˜ã€‚"
                )
            ])

            formatted_prompt = prompt.format_messages(
                topic=request.topic,
                target_audience=request.target_audience,
                post_type=request.post_type,
                tone=request.tone,
                max_length=request.max_length,
                include_emoji="æ˜¯" if request.include_emoji else "å¦"
            )

            response = self.llm.invoke(formatted_prompt)
            state["generated_content"] = response.content.strip()

        except Exception as e:
            state["errors"].append(f"å…§å®¹ç”Ÿæˆå¤±æ•—: {str(e)}")
            state["generated_content"] = f"åˆ†äº«é—œæ–¼{state['request'].topic}çš„ç²¾å½©å…§å®¹..."

        return state

    def generate_hashtags(self, state: AIPostState) -> AIPostState:
        """ç”Ÿæˆç›¸é—œæ¨™ç±¤"""
        try:
            if not state["request"].include_hashtags:
                state["hashtags"] = []
                return state

            request = state["request"]

            prompt = ChatPromptTemplate.from_messages([
                SystemMessagePromptTemplate.from_template(
                    "ä½ æ˜¯ä¸€ä½ç¤¾ç¾¤åª’é«”æ¨™ç±¤å°ˆå®¶ï¼Œå°ˆé–€ç‚º Facebook è²¼æ–‡ç”Ÿæˆæœ‰æ•ˆçš„æ¨™ç±¤ã€‚"
                ),
                HumanMessagePromptTemplate.from_template(
                    "è«‹ç‚ºä»¥ä¸‹ä¸»é¡Œç”Ÿæˆ 5-10 å€‹ç›¸é—œçš„ Facebook æ¨™ç±¤ï¼š\n\n"
                    "ä¸»é¡Œï¼š{topic}\n"
                    "ç›®æ¨™å—çœ¾ï¼š{target_audience}\n"
                    "è²¼æ–‡é¡å‹ï¼š{post_type}\n\n"
                    "è¦æ±‚ï¼š\n"
                    "1. æ¨™ç±¤è¦èˆ‡ä¸»é¡Œé«˜åº¦ç›¸é—œ\n"
                    "2. è€ƒæ…®ç›®æ¨™å—çœ¾çš„æœå°‹ç¿’æ…£\n"
                    "3. æ··åˆç†±é–€å’Œåˆ©åŸºæ¨™ç±¤\n"
                    "4. ä½¿ç”¨ç¹é«”ä¸­æ–‡å’Œè‹±æ–‡\n"
                    "5. æ¯å€‹æ¨™ç±¤å‰åŠ ä¸Š # ç¬¦è™Ÿ\n\n"
                    "è«‹ä»¥é€—è™Ÿåˆ†éš”çš„æ ¼å¼å›å‚³æ¨™ç±¤ï¼Œä¾‹å¦‚ï¼š#æ¨™ç±¤1, #æ¨™ç±¤2, #æ¨™ç±¤3"
                )
            ])

            formatted_prompt = prompt.format_messages(
                topic=request.topic,
                target_audience=request.target_audience,
                post_type=request.post_type
            )

            response = self.llm.invoke(formatted_prompt)
            hashtags_text = response.content.strip()

            # è§£ææ¨™ç±¤
            hashtags = [tag.strip() for tag in hashtags_text.split(',')]
            # ç¢ºä¿æ¯å€‹æ¨™ç±¤éƒ½æœ‰ # ç¬¦è™Ÿ
            hashtags = [tag if tag.startswith('#') else f'#{tag}' for tag in hashtags]
            state["hashtags"] = hashtags

        except Exception as e:
            state["errors"].append(f"æ¨™ç±¤ç”Ÿæˆå¤±æ•—: {str(e)}")
            state["hashtags"] = [f"#{state['request'].topic}"]

        return state

class ImageGenerator:
    """AI åœ–åƒç”Ÿæˆå™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.openai_client = None
        if config.get("openai_api_key"):
            import openai
            self.openai_client = openai.OpenAI(api_key=config["openai_api_key"])

    def generate_image_prompt(self, state: AIPostState) -> AIPostState:
        """ç”Ÿæˆåœ–åƒæç¤ºè©"""
        try:
            if not state["request"].generate_image:
                return state

            request = state["request"]

            # ä½¿ç”¨ LLM ç”Ÿæˆåœ–åƒæç¤ºè©
            llm = PostContentGenerator(self.config).llm

            prompt = ChatPromptTemplate.from_messages([
                SystemMessagePromptTemplate.from_template(
                    "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ AI åœ–åƒç”Ÿæˆæç¤ºè©å°ˆå®¶ï¼Œå°ˆé–€ç‚ºç¤¾ç¾¤åª’é«”è²¼æ–‡å‰µä½œé«˜è³ªé‡çš„åœ–åƒç”Ÿæˆæç¤ºè©ã€‚"
                ),
                HumanMessagePromptTemplate.from_template(
                    "è«‹ç‚ºä»¥ä¸‹ Facebook è²¼æ–‡å‰µä½œä¸€å€‹è©³ç´°çš„åœ–åƒç”Ÿæˆæç¤ºè©ï¼š\n\n"
                    "è²¼æ–‡ä¸»é¡Œï¼š{topic}\n"
                    "è²¼æ–‡å…§å®¹ï¼š{content}\n"
                    "ç›®æ¨™å—çœ¾ï¼š{target_audience}\n"
                    "åœ–åƒé¢¨æ ¼ï¼š{image_style}\n\n"
                    "è¦æ±‚ï¼š\n"
                    "1. æç¤ºè©è¦è©³ç´°ä¸”å…·é«”\n"
                    "2. åŒ…å«è¦–è¦ºé¢¨æ ¼ã€è‰²å½©ã€æ§‹åœ–ç­‰å…ƒç´ \n"
                    "3. ç¢ºä¿åœ–åƒèˆ‡è²¼æ–‡å…§å®¹é«˜åº¦ç›¸é—œ\n"
                    "4. è€ƒæ…®ç›®æ¨™å—çœ¾çš„å–œå¥½\n"
                    "5. ä½¿ç”¨è‹±æ–‡æ’°å¯«æç¤ºè©\n"
                    "6. é¿å…åŒ…å«æ–‡å­—å…§å®¹\n\n"
                    "åªå›å‚³æç¤ºè©å…§å®¹ï¼Œä¸éœ€è¦å…¶ä»–èªªæ˜ã€‚"
                )
            ])

            formatted_prompt = prompt.format_messages(
                topic=request.topic,
                content=state.get("generated_content", ""),
                target_audience=request.target_audience,
                image_style=request.image_style
            )

            response = llm.invoke(formatted_prompt)
            state["image_prompt"] = response.content.strip()

        except Exception as e:
            state["errors"].append(f"åœ–åƒæç¤ºè©ç”Ÿæˆå¤±æ•—: {str(e)}")
            state["image_prompt"] = f"Beautiful {state['request'].topic} illustration"

        return state

    def generate_image(self, state: AIPostState) -> AIPostState:
        """ç”Ÿæˆåœ–åƒ"""
        try:
            if not state["request"].generate_image or not state.get("image_prompt"):
                return state

            if not self.openai_client:
                state["errors"].append("åœ–åƒç”Ÿæˆéœ€è¦ OpenAI API Key")
                return state

            # ä½¿ç”¨ DALL-E 3 ç”Ÿæˆåœ–åƒ
            response = self.openai_client.images.generate(
                model=self.config.get("image_model", "dall-e-3"),
                prompt=state["image_prompt"],
                size="1024x1024",
                quality="standard",
                n=1,
            )

            image_url = response.data[0].url

            # ä¸‹è¼‰ä¸¦ä¿å­˜åœ–åƒ
            image_response = requests.get(image_url)
            if image_response.status_code == 200:
                # ç”Ÿæˆå”¯ä¸€çš„æª”æ¡ˆåç¨±
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"generated_{timestamp}.png"
                filepath = GENERATED_IMAGES_DIR / filename

                with open(filepath, 'wb') as f:
                    f.write(image_response.content)

                state["image_url"] = str(filepath)
                state["image_description"] = state["image_prompt"]
            else:
                state["errors"].append("åœ–åƒä¸‹è¼‰å¤±æ•—")

        except Exception as e:
            state["errors"].append(f"åœ–åƒç”Ÿæˆå¤±æ•—: {str(e)}")

        return state

class EngagementPredictor:
    """äº’å‹•é æ¸¬å™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.llm = PostContentGenerator(config).llm

    def predict_engagement(self, state: AIPostState) -> AIPostState:
        """é æ¸¬è²¼æ–‡äº’å‹•æ•ˆæœ"""
        try:
            prompt = ChatPromptTemplate.from_messages([
                SystemMessagePromptTemplate.from_template(
                    "ä½ æ˜¯ä¸€ä½ç¤¾ç¾¤åª’é«”æ•¸æ“šåˆ†æå°ˆå®¶ï¼Œå°ˆé–€é æ¸¬ Facebook è²¼æ–‡çš„äº’å‹•æ•ˆæœã€‚"
                ),
                HumanMessagePromptTemplate.from_template(
                    "è«‹åˆ†æä»¥ä¸‹ Facebook è²¼æ–‡çš„é æœŸäº’å‹•æ•ˆæœï¼š\n\n"
                    "æ¨™é¡Œï¼š{title}\n"
                    "å…§å®¹ï¼š{content}\n"
                    "æ¨™ç±¤ï¼š{hashtags}\n"
                    "ç›®æ¨™å—çœ¾ï¼š{target_audience}\n"
                    "è²¼æ–‡é¡å‹ï¼š{post_type}\n\n"
                    "è«‹é æ¸¬ä»¥ä¸‹æŒ‡æ¨™çš„å¾—åˆ†ï¼ˆ1-10åˆ†ï¼‰ï¼š\n"
                    "1. æŒ‰è®šç‡\n"
                    "2. ç•™è¨€ç‡\n"
                    "3. åˆ†äº«ç‡\n"
                    "4. é»æ“Šç‡\n"
                    "5. æ•´é«”äº’å‹•ç‡\n\n"
                    "è«‹ä»¥ JSON æ ¼å¼å›å‚³çµæœï¼Œä¾‹å¦‚ï¼š\n"
                    "{{\n"
                    "  \"likes\": 7.5,\n"
                    "  \"comments\": 6.0,\n"
                    "  \"shares\": 5.5,\n"
                    "  \"clicks\": 8.0,\n"
                    "  \"overall\": 7.0\n"
                    "}}"
                )
            ])

            formatted_prompt = prompt.format_messages(
                title=state.get("generated_title", ""),
                content=state.get("generated_content", ""),
                hashtags=", ".join(state.get("hashtags", [])),
                target_audience=state["request"].target_audience,
                post_type=state["request"].post_type
            )

            response = self.llm.invoke(formatted_prompt)

            # è§£æ JSON å›æ‡‰
            try:
                engagement_data = json.loads(response.content.strip())
                state["engagement_prediction"] = engagement_data
            except json.JSONDecodeError:
                # å¦‚æœç„¡æ³•è§£æ JSONï¼Œä½¿ç”¨é è¨­å€¼
                state["engagement_prediction"] = {
                    "likes": 6.0,
                    "comments": 5.0,
                    "shares": 4.5,
                    "clicks": 6.5,
                    "overall": 5.5
                }

        except Exception as e:
            state["errors"].append(f"äº’å‹•é æ¸¬å¤±æ•—: {str(e)}")
            state["engagement_prediction"] = {
                "likes": 5.0,
                "comments": 5.0,
                "shares": 5.0,
                "clicks": 5.0,
                "overall": 5.0
            }

        return state

    def generate_optimization_tips(self, state: AIPostState) -> AIPostState:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        try:
            prompt = ChatPromptTemplate.from_messages([
                SystemMessagePromptTemplate.from_template(
                    "ä½ æ˜¯ä¸€ä½ç¤¾ç¾¤åª’é«”å„ªåŒ–å°ˆå®¶ï¼Œå°ˆé–€æä¾›æé«˜ Facebook è²¼æ–‡æ•ˆæœçš„å»ºè­°ã€‚"
                ),
                HumanMessagePromptTemplate.from_template(
                    "è«‹åˆ†æä»¥ä¸‹ Facebook è²¼æ–‡ä¸¦æä¾› 3-5 å€‹å…·é«”çš„å„ªåŒ–å»ºè­°ï¼š\n\n"
                    "æ¨™é¡Œï¼š{title}\n"
                    "å…§å®¹ï¼š{content}\n"
                    "æ¨™ç±¤ï¼š{hashtags}\n"
                    "é æ¸¬äº’å‹•åˆ†æ•¸ï¼š{engagement_scores}\n\n"
                    "è«‹æä¾›å…·é«”ã€å¯è¡Œçš„å„ªåŒ–å»ºè­°ï¼Œæ¯å€‹å»ºè­°ç”¨ä¸€è¡Œè¡¨ç¤ºï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚\n"
                    "å»ºè­°æ‡‰è©²æ¶µè“‹å…§å®¹ã€æ™‚æ©Ÿã€äº’å‹•ç­‰æ–¹é¢ã€‚"
                )
            ])

            formatted_prompt = prompt.format_messages(
                title=state.get("generated_title", ""),
                content=state.get("generated_content", ""),
                hashtags=", ".join(state.get("hashtags", [])),
                engagement_scores=str(state.get("engagement_prediction", {}))
            )

            response = self.llm.invoke(formatted_prompt)

            # å°‡å›æ‡‰åˆ†å‰²ç‚ºå»ºè­°åˆ—è¡¨
            tips = [tip.strip() for tip in response.content.strip().split('\n') if tip.strip()]
            # ç§»é™¤ç·¨è™Ÿ
            tips = [tip.replace(f"{i+1}.", "").replace(f"{i+1}ã€", "").strip()
                   for i, tip in enumerate(tips)]

            state["optimization_tips"] = tips[:5]  # æœ€å¤š 5 å€‹å»ºè­°

        except Exception as e:
            state["errors"].append(f"å„ªåŒ–å»ºè­°ç”Ÿæˆå¤±æ•—: {str(e)}")
            state["optimization_tips"] = [
                "è€ƒæ…®åœ¨è²¼æ–‡ä¸­åŠ å…¥æ›´å¤šäº’å‹•å…ƒç´ ",
                "å˜—è©¦åœ¨æœ€ä½³æ™‚é–“ç™¼å¸ƒè²¼æ–‡",
                "ä½¿ç”¨æ›´å¸å¼•äººçš„è¦–è¦ºå…§å®¹"
            ]

        return state

class AIPostWorkflow:
    """AI è²¼æ–‡ç”Ÿæˆå·¥ä½œæµç¨‹"""

    def __init__(self):
        self.config = AIConfigManager.load_config()
        self.content_generator = PostContentGenerator(self.config)
        self.image_generator = ImageGenerator(self.config)
        self.engagement_predictor = EngagementPredictor(self.config)
        self.workflow = self._build_workflow()

    def _build_workflow(self) -> StateGraph:
        """æ§‹å»º LangGraph å·¥ä½œæµç¨‹"""
        workflow = StateGraph(AIPostState)

        # æ·»åŠ ç¯€é»
        workflow.add_node("generate_title", self.content_generator.generate_title)
        workflow.add_node("generate_content", self.content_generator.generate_content)
        workflow.add_node("generate_hashtags", self.content_generator.generate_hashtags)
        workflow.add_node("generate_image_prompt", self.image_generator.generate_image_prompt)
        workflow.add_node("generate_image", self.image_generator.generate_image)
        workflow.add_node("predict_engagement", self.engagement_predictor.predict_engagement)
        workflow.add_node("generate_tips", self.engagement_predictor.generate_optimization_tips)

        # è¨­ç½®å·¥ä½œæµç¨‹
        workflow.set_entry_point("generate_title")
        workflow.add_edge("generate_title", "generate_content")
        workflow.add_edge("generate_content", "generate_hashtags")
        workflow.add_edge("generate_hashtags", "generate_image_prompt")
        workflow.add_edge("generate_image_prompt", "generate_image")
        workflow.add_edge("generate_image", "predict_engagement")
        workflow.add_edge("predict_engagement", "generate_tips")
        workflow.add_edge("generate_tips", END)

        return workflow.compile()

    def generate_post(self, request: PostGenerationRequest) -> GeneratedPost:
        """ç”Ÿæˆå®Œæ•´çš„è²¼æ–‡"""
        # åˆå§‹åŒ–ç‹€æ…‹
        initial_state: AIPostState = {
            "request": request,
            "generated_content": "",
            "generated_title": "",
            "hashtags": [],
            "optimization_tips": [],
            "engagement_prediction": {},
            "image_prompt": None,
            "image_url": None,
            "image_description": None,
            "errors": []
        }

        # åŸ·è¡Œå·¥ä½œæµç¨‹
        try:
            final_state = self.workflow.invoke(initial_state)

            # æ§‹å»ºå›å‚³çµæœ
            result = GeneratedPost(
                title=final_state.get("generated_title", ""),
                content=final_state.get("generated_content", ""),
                hashtags=final_state.get("hashtags", []),
                predicted_engagement=final_state.get("engagement_prediction", {}),
                optimization_tips=final_state.get("optimization_tips", []),
                image_url=final_state.get("image_url"),
                image_description=final_state.get("image_description"),
                generation_metadata={
                    "errors": final_state.get("errors", []),
                    "generation_time": datetime.now().isoformat(),
                    "model_used": self.config.get("default_model", "unknown"),
                    "image_generated": final_state.get("image_url") is not None
                }
            )

            return result

        except Exception as e:
            # å¦‚æœå·¥ä½œæµç¨‹å¤±æ•—ï¼Œè¿”å›åŸºæœ¬çµæœ
            return GeneratedPost(
                title=f"é—œæ–¼{request.topic}çš„åˆ†äº«",
                content=f"æƒ³è¦åˆ†äº«ä¸€äº›é—œæ–¼{request.topic}çš„ç²¾å½©å…§å®¹...",
                hashtags=[f"#{request.topic}"],
                predicted_engagement={"overall": 5.0},
                optimization_tips=["è«‹æª¢æŸ¥ AI é…ç½®è¨­å®š"],
                generation_metadata={
                    "errors": [f"ç”Ÿæˆéç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}"],
                    "generation_time": datetime.now().isoformat()
                }
            )

def show_ai_config():
    """é¡¯ç¤º AI é…ç½®é é¢"""
    st.subheader("ğŸ¤– AI é…ç½®è¨­å®š")

    config = AIConfigManager.load_config()

    with st.form("ai_config_form"):
        st.markdown("### API é‡‘é‘°è¨­å®š")

        openai_key = st.text_input(
            "OpenAI API Key",
            value=config.get("openai_api_key", ""),
            type="password",
            help="ç”¨æ–¼ GPT æ¨¡å‹å’Œ DALL-E åœ–åƒç”Ÿæˆ"
        )

        anthropic_key = st.text_input(
            "Anthropic API Key",
            value=config.get("anthropic_api_key", ""),
            type="password",
            help="ç”¨æ–¼ Claude æ¨¡å‹ (å¯é¸)"
        )

        st.markdown("### æ¨¡å‹è¨­å®š")

        col1, col2 = st.columns(2)
        with col1:
            default_model = st.selectbox(
                "é è¨­èªè¨€æ¨¡å‹",
                ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo", "claude-3-sonnet-20240229"],
                index=0 if config.get("default_model") == "gpt-4o-mini" else 0
            )

        with col2:
            image_model = st.selectbox(
                "åœ–åƒç”Ÿæˆæ¨¡å‹",
                ["dall-e-3", "dall-e-2"],
                index=0 if config.get("image_model") == "dall-e-3" else 0
            )

        st.markdown("### ç”Ÿæˆåƒæ•¸")

        col1, col2, col3 = st.columns(3)
        with col1:
            temperature = st.slider(
                "å‰µæ„åº¦ (Temperature)",
                min_value=0.0,
                max_value=1.0,
                value=config.get("temperature", 0.7),
                step=0.1,
                help="æ•¸å€¼è¶Šé«˜è¶Šæœ‰å‰µæ„ï¼Œè¶Šä½è¶Šç©©å®š"
            )

        with col2:
            max_tokens = st.number_input(
                "æœ€å¤§ç”Ÿæˆé•·åº¦",
                min_value=100,
                max_value=4000,
                value=config.get("max_tokens", 1000),
                step=100
            )

        with col3:
            enable_image = st.checkbox(
                "å•Ÿç”¨åœ–åƒç”Ÿæˆ",
                value=config.get("enable_image_generation", True)
            )

        if st.form_submit_button("ğŸ’¾ ä¿å­˜é…ç½®", type="primary"):
            new_config = {
                "openai_api_key": openai_key,
                "anthropic_api_key": anthropic_key,
                "default_model": default_model,
                "image_model": image_model,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "enable_image_generation": enable_image
            }

            AIConfigManager.save_config(new_config)
            st.success("âœ… é…ç½®å·²ä¿å­˜")
            st.rerun()

# å°å‡ºä¸»è¦é¡åˆ¥å’Œå‡½æ•¸
__all__ = [
    "PostGenerationRequest",
    "GeneratedPost",
    "AIPostWorkflow",
    "AIConfigManager",
    "show_ai_config"
]